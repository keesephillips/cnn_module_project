{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "import splitfolders\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('./data'):\n",
        "    os.mkdir('./data')\n",
        "if not os.path.exists('data/raw'):\n",
        "    url = 'http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar'\n",
        "    urllib.request.urlretrieve(url,filename='data/dogs.tar')\n",
        "    # Open the tar file\n",
        "    with tarfile.open('data/dogs.tar', 'r') as tar:\n",
        "        # Extract all files to the 'data/raw' directory\n",
        "        tar.extractall(path='data/raw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for subdir, dirs, files in os.walk(os.getcwd() + '/data/raw'):\n",
        "    for file in files:\n",
        "        filepath = subdir + os.sep + file\n",
        "\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            \n",
        "            folder = filepath.split(\"\\\\\")[-2].split(\"-\")[1]\n",
        "\n",
        "            if not os.path.isdir(os.getcwd() + \"/data/raw/images_folder/\" + folder): \n",
        "                os.makedirs(os.getcwd() + \"/data/raw/images_folder/\" + folder) \n",
        "            \n",
        "            os.rename(filepath, os.getcwd() + \"/data/raw/images_folder/\" + folder + \"/\" + filepath.split(\"\\\\\")[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splitfolders.ratio('data/raw/images_folder', output='data/processed', seed=1337, ratio=(.8, 0.0,0.2)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_image_dict(img, breed):\n",
        "    i = 0 \n",
        "    image_dict= {}\n",
        "    for i_row in range(len(img)):\n",
        "        for i_col in img[i_row]:\n",
        "            image_dict[f'pixel_{i}'] = i_col\n",
        "            i+=1\n",
        "    image_dict['Breed'] = breed\n",
        "    \n",
        "    return image_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ordinal_encode(X,cols):\n",
        "    \"\"\"\n",
        "    Takes a dataframe as an input and applies ordinal encoding to the \n",
        "    specified columns\n",
        "\n",
        "    Inputs:\n",
        "        X(pd.DataFrame): dataframe\n",
        "        cols(list): list of columns to ordinal encode\n",
        "    \n",
        "    Return:\n",
        "        X(pd.DataFrame): dataframe\n",
        "        enc: ordinal encoder\n",
        "    \"\"\"\n",
        "    enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "    # Fit the encoder on training data and transform it.  We can also use it to transform test data\n",
        "    X[cols] = enc.fit_transform(X[cols])\n",
        "    return X,enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_array = []\n",
        "for subdir, dirs, files in os.walk(os.getcwd() + '/data/processed/val/'):\n",
        "    for file in files:\n",
        "        filepath = subdir + os.sep + file\n",
        "\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            \n",
        "            breed = filepath.split(\"\\\\\")[-2].split('/')[-1]\n",
        "\n",
        "            # Read your images\n",
        "            img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, (225, 225))\n",
        "            img_array.append(create_image_dict(img, breed))\n",
        "\n",
        "# Create an empty DataFrame\n",
        "df = pd.DataFrame(img_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df, enc = ordinal_encode(df, ['Breed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your feature vectors (extracted from images) and labels\n",
        "X = df.drop(columns='Breed')\n",
        "y= df['Breed']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "resnet50 = torch.load('models/resnet50.pt',map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = 'output'\n",
        "\n",
        "# Set up transformations for training and validation (test) data\n",
        "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
        "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create Datasets for training and validation sets\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
        "                                          data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
        "                                          data_transforms['val'])\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "# Set up dict for dataloaders\n",
        "dataloaders = {'train':train_loader,'val':val_loader}\n",
        "\n",
        "# Store size of training and validation sets\n",
        "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
        "# Get class names associated with labels\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a batch of predictions\n",
        "def visualize_results(model,dataloader,device):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Get a batch of validation images\n",
        "        images, labels = next(iter(val_loader))\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Get predictions\n",
        "        _,preds = torch.max(model(images), 1)\n",
        "        preds = np.squeeze(preds.cpu().numpy())\n",
        "        images = images.cpu().numpy()\n",
        "\n",
        "    # Plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    for idx in np.arange(len(preds)):\n",
        "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
        "        image = images[idx]\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(\"{} ({})\".format(class_names[preds[idx]], class_names[labels[idx]]),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "    return\n",
        "\n",
        "visualize_results(resnet50,val_loader,device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aipi540",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
